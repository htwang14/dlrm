Using 1 GPU(s)...
Reading pre-processed data=/hdd3/jiayi/kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined train indices...
Randomized indices across days ...
Split data according to indices...
Reading pre-processed data=/hdd3/jiayi/kaggle/kaggleAdDisplayChallenge_processed.npz
Sparse fea = 26, Dense fea = 13
Defined test indices...
Randomized indices across days ...
Split data according to indices...
Loading saved model /hdd3/jiayi/result/baseline_cat_extended/best.pt
./masks/l1/60421_0.2.pkl
Parameter containing:
tensor([[-9.21063e-03,  2.35622e-02,  7.41505e-02,  ...,  9.61125e-02,
          2.09606e-01,  6.32213e-02],
        [-1.24354e-03,  4.94167e-02, -3.67557e-02,  ...,  3.33919e-02,
          5.37396e-02, -1.67116e-03],
        [-1.51979e-02,  3.31832e-02,  3.30242e-02,  ...,  7.14823e-02,
          5.56811e-02, -1.90414e-02],
        ...,
        [ 4.51751e-02, -1.64136e-02, -3.40707e-02,  ..., -3.48369e-02,
          4.09853e-02, -8.54545e-03],
        [ 1.51459e-02, -3.82268e-02, -4.05396e-02,  ..., -2.45011e-02,
          9.62551e-02, -2.50682e-02],
        [-6.76652e-02, -1.00234e-02,  8.76999e-02,  ..., -1.75098e-02,
         -2.52655e-03,  1.79930e-04]], device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[ 0.02823, -0.00530, -0.10349,  ..., -0.01247, -0.00536, -0.03096],
        [ 0.10183, -0.04460,  0.11788,  ..., -0.05907,  0.06480,  0.02069],
        [ 0.00914, -0.07403, -0.09099,  ...,  0.05036, -0.10369, -0.01552],
        ...,
        [ 0.02590, -0.06017, -0.10099,  ...,  0.01686, -0.01286,  0.08715],
        [-0.04397, -0.04752,  0.01081,  ...,  0.01180,  0.01307,  0.07666],
        [-0.06977,  0.02229, -0.02513,  ..., -0.00696, -0.01426,  0.05312]],
       device='cuda:0', requires_grad=True)
Parameter containing:
tensor([[-0.04793,  0.41961, -0.19631,  0.06881, -0.29929, -0.16739, -0.02404,
         -0.13845, -0.06842, -0.00121,  0.20477, -0.28498,  0.16393, -0.14679,
         -0.16652,  0.28134, -0.09440,  0.22102, -0.03957, -0.18589, -0.13299,
         -0.24615,  0.36356, -0.10805, -0.12033,  0.25787, -0.00226,  0.13569,
          0.03317, -0.17104, -0.22188,  0.09421, -0.25376,  0.29316, -0.17283,
         -0.06768, -0.10892,  0.10163,  0.01968,  0.01074, -0.30312, -0.14441,
         -0.11523, -0.04498, -0.00281,  0.19460,  0.13895,  0.49999,  0.04463,
         -0.05965, -0.26648, -0.00725,  0.18040, -0.00957,  0.13562, -0.19069,
         -0.18433,  0.22868, -0.07889,  0.29657, -0.06484, -0.23197,  0.20269,
         -0.10128, -0.12203, -0.12039, -0.36337,  0.22768, -0.29056,  0.15548,
         -0.06322,  0.06204,  0.08442, -0.29194, -0.27870, -0.12566, -0.20913,
         -0.14239, -0.24024, -0.19743, -0.29289,  0.05039,  0.06291,  0.01234,
          0.42078, -0.32963, -0.24348,  0.11917, -0.24023,  0.04162,  0.55770,
          0.07062, -0.13685, -0.39434,  0.00232,  0.35660,  0.07798, -0.17659,
         -0.06826,  0.30829, -0.06055,  0.38123,  0.07397,  0.17821,  0.01759,
         -0.17093,  0.24643,  0.15208,  0.08607, -0.26154, -0.00566, -0.21403,
         -0.06165,  0.10757,  0.15540,  0.73753,  0.18258, -0.00884,  0.21163,
          0.29178,  0.53282,  0.15333, -0.17217, -0.11537,  0.03513, -0.13796,
          0.24142,  0.22519,  0.09365,  0.25709,  0.05023, -0.19053, -0.11164,
          0.11788,  0.07738,  0.15560, -0.26783,  0.22406,  0.02815,  0.48529,
         -0.00556, -0.27170, -0.09956, -0.03344,  0.36039, -0.19032, -0.25007,
         -0.11286,  0.14397,  0.10265,  0.49326, -0.23058, -0.10914, -0.09312,
          0.08417,  0.29787, -0.06542, -0.10298,  0.66701, -0.08417,  0.21173,
         -0.04925,  0.12068,  0.20663, -0.17499,  0.03010,  0.09084,  0.11066,
         -0.16200,  0.26702,  0.15842, -0.29074,  0.30321,  0.13354, -0.15361,
          0.17425,  0.07253, -0.07097, -0.07737, -0.26456,  0.04531,  0.00324,
          0.07907,  0.04599,  0.27491, -0.12857, -0.19316,  0.13306, -0.27142,
          0.14621, -0.04703,  0.14645, -0.09484, -0.26181, -0.31818, -0.06451,
         -0.83048,  0.22834,  0.03131,  0.20753, -0.16130, -0.00936,  0.12208,
         -0.01302, -0.03439,  0.17316,  0.15583, -0.21255,  0.09348, -0.15065,
          0.28561, -0.40276, -0.10929,  0.14387, -0.14281, -0.17697,  0.08719,
         -0.16426, -0.16075, -0.00199, -0.10071,  0.01786,  0.12386, -0.17975,
         -0.03387,  0.23977,  0.01235, -0.02251, -0.00910,  0.13000, -0.14797,
          0.05884, -0.19630, -0.13344, -0.06623, -0.00885, -0.17473,  0.27677,
         -0.20253,  0.30472, -0.04121, -0.14895,  0.17534,  0.25896, -0.05768,
          0.17327,  0.12725,  0.11039,  0.11023, -0.23108,  0.47379, -0.13417,
         -0.18366, -0.51948, -0.14140, -0.30340]], device='cuda:0',
       requires_grad=True)
mask: torch.Size([512, 432]) True
mask: torch.Size([256, 512]) True
mask: torch.Size([1, 256]) True
Saved at: epoch = 0/3, batch = 306176/306969, ntbatch = 200
Training state: loss = 0.439219, accuracy = 79.727 %
Testing state: loss = 0.450454, accuracy = 78.958 %
time/loss/accuracy (if enabled):
skip_upto_epoch: 0
args.nepochs: 2
k: 0
Finished training it 306969/306969 of epoch 0, 18.46 ms/it, loss 0.457496, accuracy 78.608 %
Saving model to results/l1/60421_0.2/model.pth
Testing at - 306969/306969 of epoch 0, loss 0.456977, accuracy 78.571 %, best 78.571 %
Finished training it 1024/306969 of epoch 1, 18.79 ms/it, loss 0.448068, accuracy 79.163 %
Saving model to results/l1/60421_0.2/model.pth
Testing at - 1024/306969 of epoch 0, loss 0.454468, accuracy 78.717 %, best 78.717 %
Finished training it 2048/306969 of epoch 1, 22.70 ms/it, loss 0.442954, accuracy 79.417 %
